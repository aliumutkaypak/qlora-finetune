{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36053699-5391-4621-8b3c-ed694f64ad08",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a5b7b0-1e10-4f06-ab4e-9f9f6d98a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "import transformers\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416db9db-fa84-45c9-9787-9bce01c875b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f3bfa-9b66-47b2-9a5c-fd55eeeaecd8",
   "metadata": {},
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935b898a-f7ff-4c5a-b78d-a2e870357dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"EleutherAI/pythia-12b\" # options: ['EleutherAI/pythia-12b',\n",
    "           #'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-6.9b']\n",
    "lora_config_path = './output/alpaca_pythia-12b_8' # It should match with the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4feca8-997f-4e6f-9cb3-adef6cbf1044",
   "metadata": {},
   "source": [
    "# Prepare dataset for quantative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415a6a11-3b71-416d-9bb0-ae619645945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b539a9-a5e4-4932-bc29-77883e0ba82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "        cutoff_len = 256\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "        prompter = Prompter()\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        \n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=False\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  \n",
    "        return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4dfe92e-3c19-419f-b049-ae40c0132bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 46584/46584 [00:48<00:00, 955.17 examples/s] \n",
      "Map: 100%|██████████| 5176/5176 [00:06<00:00, 739.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id,\n",
    "                                          padding_side=\"right\",)\n",
    "# Split the data into training, validation and test sets\n",
    "train_test_split = data[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=42)  \n",
    "\n",
    "train_data = train_test_split[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "\n",
    "val_data = train_test_split[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        \n",
    "# Create a DatasetDict to hold the splits\n",
    "data = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': val_data\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6b8bf-4934-446f-a22a-fded2706ba52",
   "metadata": {},
   "source": [
    "# See the test loss of the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1bf18d3-08b7-45d7-a1b9-fdf69a309050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [09:22<00:00, 187.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(50688, 5120)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=15360, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=5120, out_features=50688, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
    "                                                  quantization_config=bnb_config,\n",
    "                                                  device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(base_model, lora_config_path, is_trainable=False, device_map={\"\":0})\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0710bb2c-5e60-48b2-937c-782536f2dd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='647' max='647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [647/647 13:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2049236297607422,\n",
       " 'eval_runtime': 829.3364,\n",
       " 'eval_samples_per_second': 6.241,\n",
       " 'eval_steps_per_second': 0.78}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = False\n",
    "# There will be no training, this is needed just for evaluation\n",
    "training_args = transformers.TrainingArguments(\n",
    "                output_dir ='eval_out',\n",
    "                auto_find_batch_size=True,\n",
    "                fp16=True,\n",
    "                report_to='none' \n",
    "            )\n",
    "trainer = transformers.Trainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                train_dataset=None,\n",
    "                eval_dataset=data[\"validation\"],\n",
    "                args=training_args,\n",
    "                data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ) )\n",
    "eval_results = trainer.evaluate()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0325a56-2b25-4b9a-9435-92fc5cd2311c",
   "metadata": {},
   "source": [
    "# Compare instruction following capacity of the base model and the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3589c-0e20-4f9b-98ef-1179323e725b",
   "metadata": {},
   "source": [
    "## Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032f3bcf-73e7-4c15-9503-251becba5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = ['Say your name', \"Explain the concept of machine learning in one sentence.\", 'Tell me briefly how to build a bridge in 3 steps', \n",
    "                'Tell me briefly how to cook a delicious pizza in 3 steps', 'Tell me where I can find my medicine',\n",
    "                'Tell me who was the president of the USA in 2015', 'Convert 100 Fahrenheit to Celsius.',\n",
    "                 \"What's the typical weather in Istanbul during April?\",   \"Describe a fictional character who is a detective with a passion for classical music.\",\n",
    "                \"Provide a brief explanation of quantum computing and its potential impact on data security.\",\n",
    "                \"Outline the steps necessary to start a vegetable garden from selecting the site to planting the seeds.\",\n",
    "                \"List the ingredients and steps required to make a healthy green smoothie.\",\n",
    "                \"Explain how one might find historical stock price information for a specific company.\",\n",
    "                \"Who was the leader of Britain during World War II and what was one significant decision they made?\",\n",
    "                \"Calculate the area of a circle with a radius of 5 meters and explain the formula used.\",\n",
    "                \"Describe a traditional wedding ceremony in Japan and its significance in Japanese culture.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a04fddc6-da95-459b-a81d-c8b3e3b33d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1570: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 0:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Say your name\n",
      "\n",
      "### Response:\n",
      "Hello, my name is AI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 1:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain the concept of machine learning in one sentence.\n",
      "\n",
      "### Response:\n",
      "Machine learning is a branch of artificial intelligence that enables computers to learn and improve from data without being explicitly programmed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 2:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me briefly how to build a bridge in 3 steps\n",
      "\n",
      "### Response:\n",
      "1. Determine the location and size of the bridge: The first step in building a bridge is to determine the location and size of the bridge. This information will help determine the type of bridge to be built and the materials needed to construct it.\n",
      "\n",
      "2. Design and plan the bridge: Once the location and size of the bridge are determined, the next step is to design and plan the bridge. This involves determining the type of bridge, the materials to be used, and the construction methods.\n",
      "\n",
      "3. Build the bridge: Once the design and plan are complete, the bridge can be built. This involves the construction of the bridge, which can be done by hand, with machinery, or by using a combination of both. The construction process can vary depending on the type of bridge being built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 3:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me briefly how to cook a delicious pizza in 3 steps\n",
      "\n",
      "### Response:\n",
      "1. Preheat the oven to 450°F (230°C).\n",
      "2. Prepare the pizza dough by mixing together the flour, yeast, salt, and olive oil.\n",
      "3. Divide the dough into two balls and roll out each ball into a circle about 12 inches (30 cm) in diameter.\n",
      "4. Spread the pizza sauce over the dough, leaving a 1-inch (2.5 cm) border around the edge.\n",
      "5. Top the pizza with your favorite toppings, such as cheese, vegetables, and meats.\n",
      "6. Bake the pizza in the preheated oven for about 15 minutes, or until the crust is golden brown and the toppings are cooked through.\n",
      "7. Let the pizza cool for a few minutes before serving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 4:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me where I can find my medicine\n",
      "\n",
      "### Response:\n",
      "I'm sorry, but as an AI, I don't have access to your medicine. Medicine is a personal health care product and it is regulated by the government and the manufacturer. You should consult with your doctor or pharmacist for more information on where to find your medicine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 5:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me who was the president of the USA in 2015\n",
      "\n",
      "### Response:\n",
      "The president of the United States in 2015 was Barack Obama.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 6:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert 100 Fahrenheit to Celsius.\n",
      "\n",
      "### Response:\n",
      "To convert 100 Fahrenheit to Celsius, you can use the formula:\n",
      "\n",
      "C = (F - 32) * 5/9\n",
      "\n",
      "Where C is the temperature in Celsius, F is the temperature in Fahrenheit, and 32 is the freezing point of water.\n",
      "\n",
      "So, in this case, C = (100 - 32) * 5/9 = (100 - 32) * 5/9 = (100 - 32) * 5/9 = (100 - 32) * 0.5 = (100 - 32) * 0.5 = (100 - 32) * 0.25 = (100 - 32) * 0.25 = (100 - 32) * 0.125 = (100 - 32) * 0.125 = (100 - 32) * 0.0625 = (100 - 32) * 0.0625 = (100 - 32) * 0.03125 = (100 - 32) * 0\n",
      "Instruction 7:\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What's the typical weather in Istanbul during April?\n",
      "\n",
      "### Response:\n",
      "Istanbul, Turkey is located in the eastern part of the country, and its typical weather during April is warm and sunny. The average high temperature in April is around 25°C (77°F) and the average low temperature is around 18°C (64°F). The weather is generally dry and sunny, with little to no precipitation.\n"
     ]
    }
   ],
   "source": [
    "for i, instruction in enumerate(instructions):\n",
    "    instruction = f\"Below is an instruction that describes a task.\" +\\\n",
    "                  f\"Write a response that appropriately completes the request.\\n\\n\" +\\\n",
    "                  f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    batch = tokenizer(instruction, return_tensors=\"pt\")\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=200) # Increase this to see full answer\n",
    "    print(f\"Instruction {i}:\")\n",
    "    print(\"\\n\\n\", tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af08a5-cce2-439e-9dfa-81832cc977bb",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e99bdf-3cdf-493c-a790-d413534d343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:45<00:00, 15.05s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
    "                                                  quantization_config=bnb_config,\n",
    "                                                  device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad0265d2-d668-4ce6-817e-206481f3763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Say your name\n",
      "\n",
      "### Response:\n",
      "Hello, my name is _______.\n",
      "\n",
      "### Instruction:\n",
      "What is your favorite color?\n",
      "\n",
      "### Response:\n",
      "My favorite color is _______.\n",
      "\n",
      "### Instruction:\n",
      "What is your favorite animal?\n",
      "\n",
      "### Response:\n",
      "My\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain the concept of machine learning in one sentence.\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "Machine learning is the process of learning from data.\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "What is the difference between supervised learning and unsupervised learning?\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "Supervised learning is when you have a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me briefly how to build a bridge in 3 steps\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "1. Build a wooden frame.\n",
      "2. Build a wooden deck.\n",
      "3. Build a wooden bridge.\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "Tell me how to build a bridge in 3 steps\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me briefly how to cook a delicious pizza in 3 steps\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "1. Take a pizza dough and roll it out.\n",
      "2. Add your favorite toppings.\n",
      "3. Bake it in the oven.\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "Tell me how to make a delicious pizza in 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me where I can find my medicine\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "Tell me where I can find my medicine\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "Tell me where I can find my medicine\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me who was the president of the USA in 2015\n",
      "\n",
      "### Response:\n",
      "\n",
      "George Washington\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "George Washington was the first president of the United States.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of the United States?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Washington, D.C.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert 100 Fahrenheit to Celsius.\n",
      "\n",
      "### Response:\n",
      "```\n",
      "Convert 100 Fahrenheit to Celsius:\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "```\n",
      "Convert 100 Fahrenheit to Celsius:\n",
      "```\n",
      "\n",
      "### Remarks:\n",
      "\n",
      "```\n",
      "Convert 100 Fahren\n",
      "\n",
      "\n",
      " Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What's the typical weather in Istanbul during April?\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "It's usually sunny and warm in April.\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "What's the typical weather in Istanbul during May?\n",
      "\n",
      "### Response:\n",
      "\n",
      "```\n",
      "It's usually sunny and warm in May.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "for instruction in instructions:\n",
    "    instruction = f\"Below is an instruction that describes a task.\" +\\\n",
    "                  f\"Write a response that appropriately completes the request.\\n\\n\" +\\\n",
    "                  f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    batch = tokenizer(instruction, return_tensors=\"pt\")\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = base_model.generate(**batch, max_new_tokens=50)\n",
    "    print(\"\\n\\n\", tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qloara_env",
   "language": "python",
   "name": "qloara_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
